{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11026337,"sourceType":"datasetVersion","datasetId":6866540},{"sourceId":11027242,"sourceType":"datasetVersion","datasetId":6867216},{"sourceId":11112585,"sourceType":"datasetVersion","datasetId":6928409},{"sourceId":11128556,"sourceType":"datasetVersion","datasetId":6940389},{"sourceId":294631,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":252300,"modelId":273772}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torchvision.utils import save_image\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\nimport os\nimport numpy as np\nimport json\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-22T17:29:48.029473Z","iopub.execute_input":"2025-03-22T17:29:48.029794Z","iopub.status.idle":"2025-03-22T17:29:53.923866Z","shell.execute_reply.started":"2025-03-22T17:29:48.029764Z","shell.execute_reply":"2025-03-22T17:29:53.923173Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"\n\nimport torchvision.transforms as transforms\n\nclass DenoisingDataset(Dataset):\n    def __init__(self, noisy_dir, clean_dir, transform=None  , json_path : str = None):\n        self.noisy_dir = noisy_dir\n        self.clean_dir = clean_dir\n        self.transform = transform\n        with open(file = json_path , mode = \"r\") as fp:\n            self.image_files = json.load(fp = fp)\n        self.resize_transform = transforms.Resize((256, 256))\n\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        # some minor tweaks\n        noisy_image_path = os.path.join(self.noisy_dir, self.image_files[idx][\"path_gt\"])\n        clean_image_path = os.path.join(self.clean_dir, self.image_files[idx][\"path_lq\"].replace(\"X4/\" , \"\"))\n\n        noisy_image = Image.open(noisy_image_path).convert(\"L\")\n        clean_image = Image.open(clean_image_path).convert(\"L\")\n\n        noisy_image = noisy_image.convert(\"RGB\")\n        clean_image = clean_image.convert(\"RGB\")\n\n        noisy_image = self.resize_transform(noisy_image)\n        clean_image = self.resize_transform(clean_image)\n\n        if self.transform:\n            noisy_image = self.transform(noisy_image)\n            clean_image = self.transform(clean_image)\n        return noisy_image, clean_image\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T17:29:53.924868Z","iopub.execute_input":"2025-03-22T17:29:53.925236Z","iopub.status.idle":"2025-03-22T17:29:53.931443Z","shell.execute_reply.started":"2025-03-22T17:29:53.925206Z","shell.execute_reply":"2025-03-22T17:29:53.930664Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from torch.utils.data import random_split\nbatch_size = 16\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize between [-1,1]\n])\n\nfull_dataset = DenoisingDataset(\n        noisy_dir=\"/kaggle/input/image-dataset-collection/HR\",\n        clean_dir=\"/kaggle/input/image-dataset-collection/train\",\n        transform=transform,\n        json_path = \"/kaggle/input/image-mapping/train_X4.json\"\n    )\n    \ntrain_size = int(0.8 * len(full_dataset))\nval_size = len(full_dataset) - train_size\n    \ntrain_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\ntrain_loader_x = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader_x = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\nprint(len(train_loader_x))\nprint(len(val_loader_x))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T17:29:54.534397Z","iopub.execute_input":"2025-03-22T17:29:54.534654Z","iopub.status.idle":"2025-03-22T17:29:54.713698Z","shell.execute_reply.started":"2025-03-22T17:29:54.534626Z","shell.execute_reply":"2025-03-22T17:29:54.712753Z"}},"outputs":[{"name":"stdout","text":"4250\n1063\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from torch.utils.data import Subset\nimport numpy as np\n\n\ndef get_subset_loader(dataset, percentage=0.1, batch_size=16, shuffle=True, num_workers=4):\n    dataset_size = len(dataset)\n    subset_size = int(dataset_size * percentage)\n    indices = np.random.choice(dataset_size, subset_size, replace=False)  # Randomly sample 10% of indices\n    subset = Subset(dataset, indices)\n    return torch.utils.data.DataLoader(subset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)  , subset\n\n# Assuming train_loader and val_loader have dataset attributes\ntrain_subset_loader , train_subset = get_subset_loader(train_loader_x.dataset, percentage=0.8, batch_size=16)\nval_subset_loader , val_subset = get_subset_loader(val_loader_x.dataset, percentage=0.05, batch_size=16)\n\nprint(len(train_subset_loader))\nprint(len(val_subset_loader))\nprint(len(train_subset))\nprint(len(val_subset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T17:29:57.533169Z","iopub.execute_input":"2025-03-22T17:29:57.533480Z","iopub.status.idle":"2025-03-22T17:29:57.543666Z","shell.execute_reply.started":"2025-03-22T17:29:57.533452Z","shell.execute_reply":"2025-03-22T17:29:57.542794Z"}},"outputs":[{"name":"stdout","text":"3400\n54\n54393\n849\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"\n\nimport os\nimport time\nimport torch\nfrom torch import nn\nimport torch.utils.data as td\nfrom abc import ABC, abstractmethod\n\n\nclass NeuralNetwork(nn.Module, ABC):\n    def __init__(self):\n        super(NeuralNetwork, self).__init__()\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n    def named_parameters(self, recurse=True):\n        nps = nn.Module.named_parameters(self)\n        for name, param in nps:\n            if not param.requires_grad:\n                continue\n            yield name, param\n\n    @abstractmethod\n    def forward(self, x):\n        pass\n\n    @abstractmethod\n    def criterion(self, y, d):\n        pass\n\n\nclass StatsManager:\n    def __init__(self):\n        self.init()\n\n    def __repr__(self):\n        return self.__class__.__name__\n\n    def init(self):\n        self.running_loss = 0\n        self.number_update = 0\n\n    def accumulate(self, loss, x=None, y=None, d=None):\n        self.running_loss += loss\n        self.number_update += 1\n\n    def summarize(self):\n        return self.running_loss / self.number_update\n\n\nclass Experiment:\n    def __init__(self, net, train_set, val_set, optimizer, stats_manager,\n                 output_dir=None, batch_size=16, perform_validation_during_training=False):\n        train_loader = td.DataLoader(train_set, batch_size=batch_size, shuffle=True,\n                                     drop_last=True, pin_memory=True)\n        val_loader = td.DataLoader(val_set, batch_size=batch_size, shuffle=False,\n                                   drop_last=True, pin_memory=True)\n        history = []\n        if output_dir is None:\n            output_dir = 'experiment_{}'.format(time.time())\n        os.makedirs(output_dir, exist_ok=True)\n        checkpoint_path = os.path.join(output_dir, \"checkpoint.pth.tar\")\n        config_path = os.path.join(output_dir, \"config.txt\")\n        locs = {k: v for k, v in locals().items() if k != 'self'}\n        self.__dict__.update(locs)\n        if os.path.isfile(config_path):\n            with open(config_path, 'r') as f:\n                if f.read().strip() != repr(self):\n                    raise ValueError(\"Cannot create this experiment: checkpoint conflict.\")\n            self.load()\n        else:\n            self.save()\n\n    @property\n    def epoch(self):\n        return len(self.history)\n\n    def setting(self):\n        return {'Net': self.net,\n                'TrainSet': self.train_set,\n                'ValSet': self.val_set,\n                'Optimizer': self.optimizer,\n                'StatsManager': self.stats_manager,\n                'BatchSize': self.batch_size,\n                'PerformValidationDuringTraining': self.perform_validation_during_training}\n\n    def __repr__(self):\n        return '\\n'.join('{}({})'.format(k, v) for k, v in self.setting().items())\n\n    def state_dict(self):\n        return {'Net': self.net.state_dict(),\n                'Optimizer': self.optimizer.state_dict(),\n                'History': self.history}\n\n    def load_state_dict(self, checkpoint):\n        self.net.load_state_dict(checkpoint['Net'])\n        self.optimizer.load_state_dict(checkpoint['Optimizer'])\n        self.history = checkpoint['History']\n        for state in self.optimizer.state.values():\n            for k, v in state.items():\n                if isinstance(v, torch.Tensor):\n                    state[k] = v.to(self.net.device)\n\n    def save(self):\n        torch.save(self.state_dict(), self.checkpoint_path)\n        with open(self.config_path, 'w') as f:\n            print(self, file=f)\n\n    def load(self):\n        checkpoint = torch.load(self.checkpoint_path, map_location=self.net.device)\n        self.load_state_dict(checkpoint)\n        del checkpoint\n\n    def run(self, num_epochs, plot=None):\n        self.net.train()\n        self.stats_manager.init()\n        start_epoch = self.epoch\n        print(\"Start/Continue training from epoch {}\".format(start_epoch))\n        if plot:\n            plot(self)\n        for epoch in range(start_epoch, num_epochs):\n            s = time.time()\n            self.stats_manager.init()\n            print(f\"Length of train loader is {len(self.train_loader)}\")\n            for x, d in self.train_loader:\n                x, d = x.to(self.net.device), d.to(self.net.device)\n                self.optimizer.zero_grad()\n                y = self.net.forward(x)\n                loss = self.net.criterion(y, d)\n                loss.backward()\n                self.optimizer.step()\n                # print(f\"loss is {loss}\")\n                with torch.no_grad():\n                    self.stats_manager.accumulate(loss.item(), x, y, d)\n            if not self.perform_validation_during_training:\n                self.history.append(self.stats_manager.summarize())\n                print(\"Epoch {} | Time: {:.2f}s | Training Loss: {:.6f}\".format(\n                    epoch, time.time() - s, self.history[-1]))\n\n            else:\n                self.history.append((self.stats_manager.summarize(), self.evaluate()))\n                # print(\"Epoch {} | Time: {:.2f}s | Training Loss: {:.6f} | Evaluation Loss: {:.6f}\".format(\n                #     self.epoch, time.time() - s, self.history[-1][0], self.history[-1][1]))\n\n                print(\"Epoch {} | Time: {:.2f}s | Training Loss: {:.6f} | Evaluation Loss: {:.6f}\".format(\n                        epoch, time.time() - s, self.history[-1][0]['loss'], self.history[-1][1]['loss']))\n            self.save()\n            if plot:\n                plot(self)\n        print(\"Finish training for {} epochs\".format(num_epochs))\n\n    def evaluate(self):\n        self.stats_manager.init()\n        self.net.eval()\n        with torch.no_grad():\n            for x, d in self.val_loader:\n                x, d = x.to(self.net.device), d.to(self.net.device)\n                y = self.net.forward(x)\n                loss = self.net.criterion(y, d)\n                self.stats_manager.accumulate(loss.item(), x, y, d)\n        self.net.train()\n        return self.stats_manager.summarize()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T17:35:35.287216Z","iopub.execute_input":"2025-03-22T17:35:35.287510Z","iopub.status.idle":"2025-03-22T17:35:35.305976Z","shell.execute_reply.started":"2025-03-22T17:35:35.287487Z","shell.execute_reply":"2025-03-22T17:35:35.305060Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import nn\n\n\ndef imshow(image, ax=plt):\n    image = image.to('cpu').numpy()\n    image = np.moveaxis(image, [0, 1, 2], [2, 0, 1])\n    image = (image + 1) / 2\n    image[image < 0] = 0\n    image[image > 1] = 1\n    h = ax.imshow(image)\n    ax.axis('off')\n    return h\n\n\ndef plot(exp, fig, axes, noisy, visu_rate=2):\n    if exp.epoch % visu_rate != 0:\n        return\n    with torch.no_grad():\n        denoised = exp.net(noisy[None].to(exp.net.device))[0]\n    axes[0][0].clear()\n    axes[0][1].clear()\n    axes[1][0].clear()\n    axes[1][1].clear()\n    imshow(noisy, ax=axes[0][0])\n    axes[0][0].set_title('Noisy image')\n\n    imshow(denoised, ax=axes[0][1])\n    axes[0][1].set_title('Denoised image')\n\n    axes[1][0].plot([exp.history[k][0]['loss']\n                     for k in range(exp.epoch)], label='training loss')\n    axes[1][0].set_ylabel('Loss')\n    axes[1][0].set_xlabel('Epoch')\n    axes[1][0].legend()\n\n    axes[1][1].plot([exp.history[k][0]['PSNR']\n                     for k in range(exp.epoch)], label='training psnr')\n    axes[1][1].set_ylabel('PSNR')\n    axes[1][1].set_xlabel('Epoch')\n    axes[1][1].legend()\n\n    plt.tight_layout()\n    fig.canvas.draw()\n\n\nclass NNRegressor(NeuralNetwork):\n\n    def __init__(self):\n        super(NNRegressor, self).__init__()\n        self.mse = nn.MSELoss()\n\n    def criterion(self, y, d):\n        return self.mse(y, d)\n\n\nclass DenoisingStatsManager(StatsManager):\n\n    def __init__(self):\n        super(DenoisingStatsManager, self).__init__()\n\n    def init(self):\n        super(DenoisingStatsManager, self).init()\n        self.running_psnr = 0\n\n    def accumulate(self, loss, x, y, d):\n        super(DenoisingStatsManager, self).accumulate(loss, x, y, d)\n        n = x.shape[0] * x.shape[1] * x.shape[2] * x.shape[3]\n        self.running_psnr += 10*torch.log10(4*n/(torch.norm(y-d)**2))\n\n    def summarize(self):\n        loss = super(DenoisingStatsManager, self).summarize()\n        psnr = self.running_psnr / self.number_update\n        print(f\"running psrn is {psnr}\")\n        return {'loss': loss, 'PSNR': psnr.cpu()}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T17:36:51.011663Z","iopub.execute_input":"2025-03-22T17:36:51.011961Z","iopub.status.idle":"2025-03-22T17:36:51.022878Z","shell.execute_reply.started":"2025-03-22T17:36:51.011940Z","shell.execute_reply":"2025-03-22T17:36:51.022042Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nimport torch.utils.data as td\nimport torchvision as tv\nimport pandas as pd\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\n\n\nclass DnCNN(NNRegressor):\n\n    def __init__(self, D, C=64):\n        super(DnCNN, self).__init__()\n        self.D = D\n\n        # convolution layers\n        self.conv = nn.ModuleList()\n        self.conv.append(nn.Conv2d(3, C, 3, padding=1))\n        self.conv.extend([nn.Conv2d(C, C, 3, padding=1) for _ in range(D)])\n        self.conv.append(nn.Conv2d(C, 3, 3, padding=1))\n        # apply He's initialization\n        for i in range(len(self.conv[:-1])):\n            nn.init.kaiming_normal_(\n                self.conv[i].weight.data, nonlinearity='relu')\n\n        # batch normalization\n        self.bn = nn.ModuleList()\n        self.bn.extend([nn.BatchNorm2d(C, C) for _ in range(D)])\n        # initialize the weights of the Batch normalization layers\n        for i in range(D):\n            nn.init.constant_(self.bn[i].weight.data, 1.25 * np.sqrt(C))\n\n    def forward(self, x):\n        D = self.D\n        h = F.relu(self.conv[0](x))\n        for i in range(D):\n            h = F.relu(self.bn[i](self.conv[i+1](h)))\n        y = self.conv[D+1](h) + x\n        return y\n\n\nclass UDnCNN(NNRegressor):\n\n    def __init__(self, D, C=64):\n        super(UDnCNN, self).__init__()\n        self.D = D\n\n        # convolution layers\n        self.conv = nn.ModuleList()\n        self.conv.append(nn.Conv2d(3, C, 3, padding=1))\n        self.conv.extend([nn.Conv2d(C, C, 3, padding=1) for _ in range(D)])\n        self.conv.append(nn.Conv2d(C, 3, 3, padding=1))\n        # apply He's initialization\n        for i in range(len(self.conv[:-1])):\n            nn.init.kaiming_normal_(\n                self.conv[i].weight.data, nonlinearity='relu')\n\n        # batch normalization\n        self.bn = nn.ModuleList()\n        self.bn.extend([nn.BatchNorm2d(C, C) for _ in range(D)])\n        # initialize the weights of the Batch normalization layers\n        for i in range(D):\n            nn.init.constant_(self.bn[i].weight.data, 1.25 * np.sqrt(C))\n\n    def forward(self, x):\n        D = self.D\n        h = F.relu(self.conv[0](x))\n        h_buff = []\n        idx_buff = []\n        shape_buff = []\n        for i in range(D//2-1):\n            shape_buff.append(h.shape)\n            h, idx = F.max_pool2d(F.relu(self.bn[i](self.conv[i+1](h))),\n                                  kernel_size=(2, 2), return_indices=True)\n            h_buff.append(h)\n            idx_buff.append(idx)\n        for i in range(D//2-1, D//2+1):\n            h = F.relu(self.bn[i](self.conv[i+1](h)))\n        for i in range(D//2+1, D):\n            j = i - (D // 2 + 1) + 1\n            h = F.max_unpool2d(F.relu(self.bn[i](self.conv[i+1]((h+h_buff[-j])/np.sqrt(2)))),\n                               idx_buff[-j], kernel_size=(2, 2), output_size=shape_buff[-j])\n        y = self.conv[D+1](h) + x\n        return y\n\n\nclass DUDnCNN(NNRegressor):\n\n    def __init__(self, D, C=64):\n        super(DUDnCNN, self).__init__()\n        self.D = D\n\n        # compute k(max_pool) and l(max_unpool)\n        k = [0]\n        k.extend([i for i in range(D//2)])\n        k.extend([k[-1] for _ in range(D//2, D+1)])\n        l = [0 for _ in range(D//2+1)]\n        l.extend([i for i in range(D+1-(D//2+1))])\n        l.append(l[-1])\n\n        # holes and dilations for convolution layers\n        holes = [2**(kl[0]-kl[1])-1 for kl in zip(k, l)]\n        dilations = [i+1 for i in holes]\n\n        # convolution layers\n        self.conv = nn.ModuleList()\n        self.conv.append(\n            nn.Conv2d(3, C, 3, padding=dilations[0], dilation=dilations[0]))\n        self.conv.extend([nn.Conv2d(C, C, 3, padding=dilations[i+1],\n                                    dilation=dilations[i+1]) for i in range(D)])\n        self.conv.append(\n            nn.Conv2d(C, 3, 3, padding=dilations[-1], dilation=dilations[-1]))\n        # apply He's initialization\n        for i in range(len(self.conv[:-1])):\n            nn.init.kaiming_normal_(\n                self.conv[i].weight.data, nonlinearity='relu')\n\n        # batch normalization\n        self.bn = nn.ModuleList()\n        self.bn.extend([nn.BatchNorm2d(C, C) for _ in range(D)])\n        # initialize the weights of the Batch normalization layers\n        for i in range(D):\n            nn.init.constant_(self.bn[i].weight.data, 1.25 * np.sqrt(C))\n\n    def forward(self, x):\n        D = self.D\n        h = F.relu(self.conv[0](x))\n        h_buff = []\n\n        for i in range(D//2 - 1):\n            torch.backends.cudnn.benchmark = True\n            h = self.conv[i+1](h)\n            torch.backends.cudnn.benchmark = False\n            h = F.relu(self.bn[i](h))\n            h_buff.append(h)\n\n        for i in range(D//2 - 1, D//2 + 1):\n            torch.backends.cudnn.benchmark = True\n            h = self.conv[i+1](h)\n            torch.backends.cudnn.benchmark = False\n            h = F.relu(self.bn[i](h))\n\n        for i in range(D//2 + 1, D):\n            j = i - (D//2 + 1) + 1\n            torch.backends.cudnn.benchmark = True\n            h = self.conv[i+1]((h + h_buff[-j]) / np.sqrt(2))\n            torch.backends.cudnn.benchmark = False\n            h = F.relu(self.bn[i](h))\n\n        y = self.conv[D+1](h) + x\n        return y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T17:38:34.685102Z","iopub.execute_input":"2025-03-22T17:38:34.685381Z","iopub.status.idle":"2025-03-22T17:38:34.706286Z","shell.execute_reply.started":"2025-03-22T17:38:34.685361Z","shell.execute_reply":"2025-03-22T17:38:34.705571Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import os\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport torch\n\nclass XYZ(Dataset):\n    def __init__(self, folder_path, img_size=(256, 256)):  # Corrected __init__\n        self.folder_path = folder_path\n        self.img_size = img_size\n        self.image_paths = [os.path.join(folder_path, f) for f in os.listdir(folder_path) \n                            if f.lower().endswith(('jpg', 'jpeg', 'png', 'bmp', 'tiff'))]\n\n        self.transform = transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize between [-1,1]\n        ])\n\n    def __len__(self):  # Corrected __len__\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):  # Corrected __getitem__\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert(\"RGB\")  \n        image = self.transform(image)\n        return image, img_path.replace(\"/kaggle/input/new-noise\", \"\")\n\ndef get_dataloader(folder_path, batch_size=32, shuffle=True, num_workers=4):\n    dataset = XYZ(folder_path)  # Now works correctly\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n    return dataloader\n\n# Example usage\nloader = get_dataloader(\"/kaggle/input/new-noise\", 8)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T17:38:38.348674Z","iopub.execute_input":"2025-03-22T17:38:38.348952Z","iopub.status.idle":"2025-03-22T17:38:38.357018Z","shell.execute_reply.started":"2025-03-22T17:38:38.348932Z","shell.execute_reply":"2025-03-22T17:38:38.356368Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":" net = DUDnCNN(6, 64)\ncheckpoint = torch.load(\"/kaggle/input/denoise/pytorch/default/1/checkpoint.pth.tar\")\n\n# Step 3: Restore the state\nnet.load_state_dict(checkpoint['Net'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T17:38:40.933685Z","iopub.execute_input":"2025-03-22T17:38:40.933959Z","iopub.status.idle":"2025-03-22T17:38:41.353336Z","shell.execute_reply.started":"2025-03-22T17:38:40.933940Z","shell.execute_reply":"2025-03-22T17:38:41.352689Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-24-6812d9ef5d62>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(\"/kaggle/input/denoise/pytorch/default/1/checkpoint.pth.tar\")\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"from torchvision.transforms import ToPILImage\n\ndef save_batch_as_png(tensor_batch, output_paths):\n    batch_size = tensor_batch.shape[0]\n\n    if tensor_batch.ndim != 4 or tensor_batch.shape[1] != 3:\n        raise ValueError(\"Tensor batch must have shape (batch_size, 3, 256, 256)\")\n    \n    if len(output_paths) != batch_size:\n        raise ValueError(f\"Number of output paths ({len(output_paths)}) must match batch size ({batch_size})\")\n\n    for i in range(batch_size):\n        tensor = tensor_batch[i]\n\n        if tensor.min() < 0 or tensor.max() <= 1:\n            tensor = (tensor * 0.5) + 0.5 \n\n        tensor = torch.clamp(tensor, 0, 1)\n        image = ToPILImage()(tensor)\n        image.save( os.path.join(\"/kaggle/working/output\" , output_paths[i]) )\n        print(f\"Saved image {i + 1}/{batch_size} to {output_paths[i]}\")\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T15:59:29.440350Z","iopub.execute_input":"2025-03-22T15:59:29.440641Z","iopub.status.idle":"2025-03-22T15:59:29.446171Z","shell.execute_reply.started":"2025-03-22T15:59:29.440619Z","shell.execute_reply":"2025-03-22T15:59:29.445309Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"#pwd","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#for itm , name in loader:\n\n   # out = net(itm)\n   # save_batch_as_png(out  , name)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T15:59:34.550538Z","iopub.execute_input":"2025-03-22T15:59:34.550830Z","iopub.status.idle":"2025-03-22T16:01:25.687230Z","shell.execute_reply.started":"2025-03-22T15:59:34.550795Z","shell.execute_reply":"2025-03-22T16:01:25.686251Z"}},"outputs":[{"name":"stdout","text":"Saved image 1/8 to /0977.png\nSaved image 2/8 to /0951.png\nSaved image 3/8 to /0000011.png\nSaved image 4/8 to /0000047.png\nSaved image 5/8 to /0917.png\nSaved image 6/8 to /0952.png\nSaved image 7/8 to /0962.png\nSaved image 8/8 to /0000016.png\nSaved image 1/8 to /0907.png\nSaved image 2/8 to /0902.png\nSaved image 3/8 to /0000044.png\nSaved image 4/8 to /0964.png\nSaved image 5/8 to /0000013.png\nSaved image 6/8 to /0000001.png\nSaved image 7/8 to /0000039.png\nSaved image 8/8 to /0000012.png\nSaved image 1/8 to /0908.png\nSaved image 2/8 to /0982.png\nSaved image 3/8 to /0959.png\nSaved image 4/8 to /0901.png\nSaved image 5/8 to /0934.png\nSaved image 6/8 to /0000096.png\nSaved image 7/8 to /0000020.png\nSaved image 8/8 to /0950.png\nSaved image 1/8 to /0000050.png\nSaved image 2/8 to /0000067.png\nSaved image 3/8 to /0000087.png\nSaved image 4/8 to /0970.png\nSaved image 5/8 to /0996.png\nSaved image 6/8 to /0938.png\nSaved image 7/8 to /0000098.png\nSaved image 8/8 to /0926.png\nSaved image 1/8 to /0928.png\nSaved image 2/8 to /0000079.png\nSaved image 3/8 to /0933.png\nSaved image 4/8 to /0998.png\nSaved image 5/8 to /0932.png\nSaved image 6/8 to /0000082.png\nSaved image 7/8 to /0936.png\nSaved image 8/8 to /0000054.png\nSaved image 1/8 to /0000046.png\nSaved image 2/8 to /0978.png\nSaved image 3/8 to /0000092.png\nSaved image 4/8 to /0971.png\nSaved image 5/8 to /0986.png\nSaved image 6/8 to /0960.png\nSaved image 7/8 to /0956.png\nSaved image 8/8 to /0987.png\nSaved image 1/8 to /0000022.png\nSaved image 2/8 to /0954.png\nSaved image 3/8 to /0967.png\nSaved image 4/8 to /0910.png\nSaved image 5/8 to /0000008.png\nSaved image 6/8 to /0923.png\nSaved image 7/8 to /0000068.png\nSaved image 8/8 to /0976.png\nSaved image 1/8 to /0906.png\nSaved image 2/8 to /0000088.png\nSaved image 3/8 to /0000066.png\nSaved image 4/8 to /0000056.png\nSaved image 5/8 to /0000026.png\nSaved image 6/8 to /0972.png\nSaved image 7/8 to /0937.png\nSaved image 8/8 to /0000078.png\nSaved image 1/8 to /0935.png\nSaved image 2/8 to /0946.png\nSaved image 3/8 to /0000099.png\nSaved image 4/8 to /0994.png\nSaved image 5/8 to /0981.png\nSaved image 6/8 to /0995.png\nSaved image 7/8 to /0000097.png\nSaved image 8/8 to /0989.png\nSaved image 1/8 to /0000091.png\nSaved image 2/8 to /0000055.png\nSaved image 3/8 to /0000003.png\nSaved image 4/8 to /0909.png\nSaved image 5/8 to /0000042.png\nSaved image 6/8 to /0965.png\nSaved image 7/8 to /0931.png\nSaved image 8/8 to /0953.png\nSaved image 1/8 to /0000065.png\nSaved image 2/8 to /0000036.png\nSaved image 3/8 to /0949.png\nSaved image 4/8 to /0999.png\nSaved image 5/8 to /0955.png\nSaved image 6/8 to /0000049.png\nSaved image 7/8 to /0000069.png\nSaved image 8/8 to /0000083.png\nSaved image 1/8 to /0000015.png\nSaved image 2/8 to /0941.png\nSaved image 3/8 to /0000034.png\nSaved image 4/8 to /0985.png\nSaved image 5/8 to /0000084.png\nSaved image 6/8 to /0000017.png\nSaved image 7/8 to /0961.png\nSaved image 8/8 to /0979.png\nSaved image 1/8 to /0968.png\nSaved image 2/8 to /0991.png\nSaved image 3/8 to /0000030.png\nSaved image 4/8 to /0975.png\nSaved image 5/8 to /0992.png\nSaved image 6/8 to /0000019.png\nSaved image 7/8 to /0912.png\nSaved image 8/8 to /0920.png\nSaved image 1/8 to /0911.png\nSaved image 2/8 to /0966.png\nSaved image 3/8 to /0000094.png\nSaved image 4/8 to /0922.png\nSaved image 5/8 to /0000061.png\nSaved image 6/8 to /0000035.png\nSaved image 7/8 to /0000080.png\nSaved image 8/8 to /0000009.png\nSaved image 1/8 to /0000064.png\nSaved image 2/8 to /0958.png\nSaved image 3/8 to /0984.png\nSaved image 4/8 to /0000010.png\nSaved image 5/8 to /0000075.png\nSaved image 6/8 to /0000023.png\nSaved image 7/8 to /0000040.png\nSaved image 8/8 to /0945.png\nSaved image 1/8 to /0914.png\nSaved image 2/8 to /0905.png\nSaved image 3/8 to /0921.png\nSaved image 4/8 to /0000062.png\nSaved image 5/8 to /0000043.png\nSaved image 6/8 to /0942.png\nSaved image 7/8 to /0000027.png\nSaved image 8/8 to /0000053.png\nSaved image 1/8 to /0973.png\nSaved image 2/8 to /0000018.png\nSaved image 3/8 to /0000005.png\nSaved image 4/8 to /0000057.png\nSaved image 5/8 to /0980.png\nSaved image 6/8 to /0988.png\nSaved image 7/8 to /0000051.png\nSaved image 8/8 to /0000025.png\nSaved image 1/8 to /0000073.png\nSaved image 2/8 to /0000060.png\nSaved image 3/8 to /0000081.png\nSaved image 4/8 to /0939.png\nSaved image 5/8 to /0000021.png\nSaved image 6/8 to /0997.png\nSaved image 7/8 to /0944.png\nSaved image 8/8 to /0957.png\nSaved image 1/8 to /0000071.png\nSaved image 2/8 to /0000085.png\nSaved image 3/8 to /0000007.png\nSaved image 4/8 to /0929.png\nSaved image 5/8 to /0000077.png\nSaved image 6/8 to /0000072.png\nSaved image 7/8 to /0943.png\nSaved image 8/8 to /0983.png\nSaved image 1/8 to /0000090.png\nSaved image 2/8 to /0000004.png\nSaved image 3/8 to /0000070.png\nSaved image 4/8 to /0000045.png\nSaved image 5/8 to /0000031.png\nSaved image 6/8 to /0927.png\nSaved image 7/8 to /1000.png\nSaved image 8/8 to /0974.png\nSaved image 1/8 to /0993.png\nSaved image 2/8 to /0000093.png\nSaved image 3/8 to /0930.png\nSaved image 4/8 to /0969.png\nSaved image 5/8 to /0000024.png\nSaved image 6/8 to /0918.png\nSaved image 7/8 to /0000041.png\nSaved image 8/8 to /0000076.png\nSaved image 1/8 to /0915.png\nSaved image 2/8 to /0000086.png\nSaved image 3/8 to /0963.png\nSaved image 4/8 to /0000029.png\nSaved image 5/8 to /0000032.png\nSaved image 6/8 to /0924.png\nSaved image 7/8 to /0916.png\nSaved image 8/8 to /0913.png\nSaved image 1/8 to /0000074.png\nSaved image 2/8 to /0000089.png\nSaved image 3/8 to /0000037.png\nSaved image 4/8 to /0000100.png\nSaved image 5/8 to /0000014.png\nSaved image 6/8 to /0000048.png\nSaved image 7/8 to /0000059.png\nSaved image 8/8 to /0948.png\nSaved image 1/8 to /0925.png\nSaved image 2/8 to /0000052.png\nSaved image 3/8 to /0904.png\nSaved image 4/8 to /0919.png\nSaved image 5/8 to /0903.png\nSaved image 6/8 to /0000038.png\nSaved image 7/8 to /0000033.png\nSaved image 8/8 to /0000028.png\nSaved image 1/8 to /0000063.png\nSaved image 2/8 to /0947.png\nSaved image 3/8 to /0990.png\nSaved image 4/8 to /0940.png\nSaved image 5/8 to /0000006.png\nSaved image 6/8 to /0000002.png\nSaved image 7/8 to /0000095.png\nSaved image 8/8 to /0000058.png\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T16:01:41.371803Z","iopub.execute_input":"2025-03-22T16:01:41.372157Z","iopub.status.idle":"2025-03-22T16:01:41.571177Z","shell.execute_reply.started":"2025-03-22T16:01:41.372128Z","shell.execute_reply":"2025-03-22T16:01:41.570144Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"'''import zipfile\nimport os\n\ndef zip_only_png(folder_path, output_zip_path):\n    with zipfile.ZipFile(output_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, _, files in os.walk(folder_path):\n            for file in files:\n                if file.lower().endswith('.png'): \n                    file_path = os.path.join(root, file)\n                    arcname = os.path.relpath(file_path, start=folder_path)  \n                    zipf.write(file_path, arcname)\n                    print(f\"Added: {file_path}\")\n    \n    print(f\"Zipped PNG files saved at: {output_zip_path}\")\nzip_only_png(\"/kaggle/working/output\", \"/kaggle/working/out.zip\")\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#cd /kaggle/working","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T16:01:49.847600Z","iopub.execute_input":"2025-03-22T16:01:49.847985Z","iopub.status.idle":"2025-03-22T16:01:49.854202Z","shell.execute_reply.started":"2025-03-22T16:01:49.847953Z","shell.execute_reply":"2025-03-22T16:01:49.853052Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"class Args():\n    def __init__(self):\n        self.output_dir = 'checkpoints1/'\n        self.num_epochs = 13\n        self.D = 6\n        self.C = 64\n        self.plot = True\n        self.model = 'dudncnn'\n        self.lr = 1e-4\n        self.image_size = (256, 256)\n        self.test_image_size = (256, 256)\n        self.batch_size = 16\n        self.sigma = 30","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T17:38:51.833464Z","iopub.execute_input":"2025-03-22T17:38:51.833807Z","iopub.status.idle":"2025-03-22T17:38:51.838406Z","shell.execute_reply.started":"2025-03-22T17:38:51.833774Z","shell.execute_reply":"2025-03-22T17:38:51.837469Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import argparse\nimport torch\nimport matplotlib.pyplot as plt\n\n\ndef run(args):\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n\n    # model\n    if args.model == 'dncnn':\n        net = DnCNN(args.D, C=args.C).to(device)\n    elif args.model == 'udncnn':\n        net = UDnCNN(args.D, C=args.C).to(device)\n    elif args.model == 'dudncnn':\n        net = DUDnCNN(args.D, C=args.C).to(device)\n    else:\n        raise NameError('Please enter: dncnn, udncnn, or dudncnn')\n\n    # optimizer\n    adam = torch.optim.Adam(net.parameters(), lr=args.lr)\n\n    # stats manager\n    stats_manager = DenoisingStatsManager()\n\n    # experiment\n    exp =  Experiment(net, train_subset, val_subset, adam, stats_manager, batch_size=args.batch_size,\n                        output_dir=args.output_dir, perform_validation_during_training=True)\n\n    # run\n    if args.plot:\n        fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(9, 7))\n        exp.run(num_epochs=args.num_epochs, plot=lambda exp: plot(exp, fig=fig, axes=axes,\n                                                noisy=val_subset[73][0]))\n    else:\n        exp.run(num_epochs=args.num_epochs)\n\n\nif __name__ == '__main__':\n    args = Args()\n    print(args)\n    run(args)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T17:38:54.192232Z","iopub.execute_input":"2025-03-22T17:38:54.192495Z","execution_failed":"2025-03-22T17:39:19.663Z"}},"outputs":[{"name":"stdout","text":"<__main__.Args object at 0x7c0472136f20>\nStart/Continue training from epoch 0\nLength of train loader is 3399\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"'''import matplotlib.pyplot as plt\nimport cv2\n\ndef plot_image(image_path):\n    \"\"\"Reads an image from the given path and displays it.\"\"\"\n    img = cv2.imread(image_path)  # Read the image\n    if img is None:\n        print(\"Error: Unable to read image. Check the file path.\")\n        return\n    \n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB (OpenCV loads images in BGR)\n    \n    plt.figure(figsize=(8, 6))\n    plt.imshow(img)\n    plt.axis(\"off\")  # Hide axes\n    plt.show()\n\n# Example usage\n# plot_image(\"path/to/your/image.jpg\")\n''''''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_image(\"/kaggle/input/image-dataset-collection/HR/HR/train/0001000/0000001.png\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_image(\"/kaggle/input/image-dataset-collection/train/train/0001000/0000001x4.png\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"list_of_files = os.listdir(\"/kaggle/input/new-noise\")\nfolder_path = \"/kaggle/input/new-noise\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T16:02:08.467481Z","iopub.execute_input":"2025-03-22T16:02:08.467771Z","iopub.status.idle":"2025-03-22T16:02:08.472222Z","shell.execute_reply.started":"2025-03-22T16:02:08.467750Z","shell.execute_reply":"2025-03-22T16:02:08.471486Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"transform_out =  transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize between [-1,1]\n        ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T16:02:12.884338Z","iopub.execute_input":"2025-03-22T16:02:12.884628Z","iopub.status.idle":"2025-03-22T16:02:12.888880Z","shell.execute_reply.started":"2025-03-22T16:02:12.884606Z","shell.execute_reply":"2025-03-22T16:02:12.888121Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"for image_path in list_of_files:\n    image = Image.open(os.path.join(folder_path, image_path)).convert(\"RGB\")  \n    transform = transforms.Compose([\n        transforms.ToTensor() \n    ])\n    image_tensor = transform(image)\n    original_resolution =list( image_tensor.shape )[::-1]\n    \n\n    in_tensor = transform_out(image).unsqueeze(dim = 0)\n    out = net(in_tensor)\n    save_tensor_as_png(out.squeeze(dim = 0) ,os.path.join(\"/kaggle/working/out_img1\" , image_path) ,   original_resolution[:-1] )\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T16:03:02.292537Z","iopub.execute_input":"2025-03-22T16:03:02.292919Z","iopub.status.idle":"2025-03-22T16:06:55.158799Z","shell.execute_reply.started":"2025-03-22T16:03:02.292886Z","shell.execute_reply":"2025-03-22T16:06:55.158055Z"}},"outputs":[{"name":"stdout","text":"Saved image to /kaggle/working/out_img1/0000089.png with resolution [1152, 864]\nSaved image to /kaggle/working/out_img1/0000014.png with resolution [872, 584]\nSaved image to /kaggle/working/out_img1/0000026.png with resolution [1280, 848]\nSaved image to /kaggle/working/out_img1/0968.png with resolution [2040, 1272]\nSaved image to /kaggle/working/out_img1/0947.png with resolution [2040, 1040]\nSaved image to /kaggle/working/out_img1/1000.png with resolution [2040, 1368]\nSaved image to /kaggle/working/out_img1/0950.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0971.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0940.png with resolution [2040, 1320]\nSaved image to /kaggle/working/out_img1/0000056.png with resolution [1520, 1016]\nSaved image to /kaggle/working/out_img1/0000019.png with resolution [752, 1008]\nSaved image to /kaggle/working/out_img1/0000030.png with resolution [1920, 1368]\nSaved image to /kaggle/working/out_img1/0000021.png with resolution [1208, 792]\nSaved image to /kaggle/working/out_img1/0000070.png with resolution [1016, 672]\nSaved image to /kaggle/working/out_img1/0000099.png with resolution [1040, 696]\nSaved image to /kaggle/working/out_img1/0905.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0974.png with resolution [2040, 1152]\nSaved image to /kaggle/working/out_img1/0985.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0000020.png with resolution [1224, 816]\nSaved image to /kaggle/working/out_img1/0958.png with resolution [1352, 2040]\nSaved image to /kaggle/working/out_img1/0910.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0917.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0000087.png with resolution [752, 1008]\nSaved image to /kaggle/working/out_img1/0914.png with resolution [2040, 1152]\nSaved image to /kaggle/working/out_img1/0000035.png with resolution [968, 608]\nSaved image to /kaggle/working/out_img1/0000005.png with resolution [1440, 960]\nSaved image to /kaggle/working/out_img1/0972.png with resolution [1352, 2040]\nSaved image to /kaggle/working/out_img1/0967.png with resolution [2040, 1320]\nSaved image to /kaggle/working/out_img1/0948.png with resolution [1536, 2040]\nSaved image to /kaggle/working/out_img1/0920.png with resolution [2040, 1368]\nSaved image to /kaggle/working/out_img1/0000042.png with resolution [1440, 960]\nSaved image to /kaggle/working/out_img1/0928.png with resolution [1352, 2040]\nSaved image to /kaggle/working/out_img1/0907.png with resolution [1496, 2040]\nSaved image to /kaggle/working/out_img1/0990.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0000068.png with resolution [2552, 1328]\nSaved image to /kaggle/working/out_img1/0933.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0000097.png with resolution [960, 632]\nSaved image to /kaggle/working/out_img1/0975.png with resolution [1352, 2040]\nSaved image to /kaggle/working/out_img1/0000033.png with resolution [1368, 720]\nSaved image to /kaggle/working/out_img1/0945.png with resolution [2040, 1536]\nSaved image to /kaggle/working/out_img1/0000067.png with resolution [1496, 776]\nSaved image to /kaggle/working/out_img1/0000027.png with resolution [2064, 1368]\nSaved image to /kaggle/working/out_img1/0000016.png with resolution [960, 696]\nSaved image to /kaggle/working/out_img1/0969.png with resolution [2040, 1032]\nSaved image to /kaggle/working/out_img1/0960.png with resolution [1536, 2040]\nSaved image to /kaggle/working/out_img1/0964.png with resolution [2040, 1152]\nSaved image to /kaggle/working/out_img1/0000065.png with resolution [864, 864]\nSaved image to /kaggle/working/out_img1/0000041.png with resolution [1440, 960]\nSaved image to /kaggle/working/out_img1/0000047.png with resolution [816, 1224]\nSaved image to /kaggle/working/out_img1/0941.png with resolution [2040, 1536]\nSaved image to /kaggle/working/out_img1/0000058.png with resolution [824, 1248]\nSaved image to /kaggle/working/out_img1/0911.png with resolution [2040, 1320]\nSaved image to /kaggle/working/out_img1/0000031.png with resolution [576, 1032]\nSaved image to /kaggle/working/out_img1/0994.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0000023.png with resolution [1224, 816]\nSaved image to /kaggle/working/out_img1/0000046.png with resolution [1344, 896]\nSaved image to /kaggle/working/out_img1/0000081.png with resolution [1352, 896]\nSaved image to /kaggle/working/out_img1/0000091.png with resolution [896, 896]\nSaved image to /kaggle/working/out_img1/0978.png with resolution [2040, 1272]\nSaved image to /kaggle/working/out_img1/0000098.png with resolution [1320, 1656]\nSaved image to /kaggle/working/out_img1/0000010.png with resolution [1152, 1608]\nSaved image to /kaggle/working/out_img1/0000003.png with resolution [3024, 2264]\nSaved image to /kaggle/working/out_img1/0999.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0000017.png with resolution [1496, 936]\nSaved image to /kaggle/working/out_img1/0000057.png with resolution [960, 1440]\nSaved image to /kaggle/working/out_img1/0000001.png with resolution [864, 864]\nSaved image to /kaggle/working/out_img1/0000025.png with resolution [1232, 816]\nSaved image to /kaggle/working/out_img1/0919.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0979.png with resolution [2040, 1368]\nSaved image to /kaggle/working/out_img1/0982.png with resolution [2040, 1272]\nSaved image to /kaggle/working/out_img1/0981.png with resolution [1944, 2040]\nSaved image to /kaggle/working/out_img1/0000088.png with resolution [720, 896]\nSaved image to /kaggle/working/out_img1/0983.png with resolution [1352, 2040]\nSaved image to /kaggle/working/out_img1/0000038.png with resolution [1104, 728]\nSaved image to /kaggle/working/out_img1/0000093.png with resolution [1472, 984]\nSaved image to /kaggle/working/out_img1/0000040.png with resolution [960, 632]\nSaved image to /kaggle/working/out_img1/0923.png with resolution [2040, 1632]\nSaved image to /kaggle/working/out_img1/0000069.png with resolution [2552, 1560]\nSaved image to /kaggle/working/out_img1/0956.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0987.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0000007.png with resolution [1232, 632]\nSaved image to /kaggle/working/out_img1/0996.png with resolution [1416, 2040]\nSaved image to /kaggle/working/out_img1/0904.png with resolution [2040, 1536]\nSaved image to /kaggle/working/out_img1/0925.png with resolution [2040, 1440]\nSaved image to /kaggle/working/out_img1/0000037.png with resolution [920, 816]\nSaved image to /kaggle/working/out_img1/0000063.png with resolution [1416, 944]\nSaved image to /kaggle/working/out_img1/0000083.png with resolution [768, 768]\nSaved image to /kaggle/working/out_img1/0913.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0939.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0908.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0926.png with resolution [2040, 920]\nSaved image to /kaggle/working/out_img1/0965.png with resolution [2040, 1304]\nSaved image to /kaggle/working/out_img1/0000053.png with resolution [864, 1296]\nSaved image to /kaggle/working/out_img1/0988.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0989.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0000059.png with resolution [912, 1296]\nSaved image to /kaggle/working/out_img1/0000096.png with resolution [1496, 992]\nSaved image to /kaggle/working/out_img1/0903.png with resolution [2040, 1536]\nSaved image to /kaggle/working/out_img1/0992.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0000054.png with resolution [1832, 1016]\nSaved image to /kaggle/working/out_img1/0936.png with resolution [2040, 1448]\nSaved image to /kaggle/working/out_img1/0995.png with resolution [2040, 1368]\nSaved image to /kaggle/working/out_img1/0980.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0961.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0902.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0901.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0000100.png with resolution [1496, 992]\nSaved image to /kaggle/working/out_img1/0000008.png with resolution [872, 872]\nSaved image to /kaggle/working/out_img1/0966.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0986.png with resolution [2040, 1536]\nSaved image to /kaggle/working/out_img1/0929.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0921.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0000034.png with resolution [1176, 744]\nSaved image to /kaggle/working/out_img1/0000029.png with resolution [1008, 744]\nSaved image to /kaggle/working/out_img1/0000043.png with resolution [992, 608]\nSaved image to /kaggle/working/out_img1/0915.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0000060.png with resolution [1152, 864]\nSaved image to /kaggle/working/out_img1/0973.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0984.png with resolution [2040, 1152]\nSaved image to /kaggle/working/out_img1/0000076.png with resolution [968, 752]\nSaved image to /kaggle/working/out_img1/0951.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0000012.png with resolution [1496, 992]\nSaved image to /kaggle/working/out_img1/0000079.png with resolution [1224, 816]\nSaved image to /kaggle/working/out_img1/0000086.png with resolution [1832, 1224]\nSaved image to /kaggle/working/out_img1/0000009.png with resolution [1472, 968]\nSaved image to /kaggle/working/out_img1/0000082.png with resolution [600, 624]\nSaved image to /kaggle/working/out_img1/0998.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0944.png with resolution [2040, 1376]\nSaved image to /kaggle/working/out_img1/0997.png with resolution [2040, 1152]\nSaved image to /kaggle/working/out_img1/0000015.png with resolution [1256, 920]\nSaved image to /kaggle/working/out_img1/0954.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0000039.png with resolution [1296, 864]\nSaved image to /kaggle/working/out_img1/0000004.png with resolution [1496, 992]\nSaved image to /kaggle/working/out_img1/0000084.png with resolution [1896, 1272]\nSaved image to /kaggle/working/out_img1/0000062.png with resolution [1400, 936]\nSaved image to /kaggle/working/out_img1/0959.png with resolution [2040, 1328]\nSaved image to /kaggle/working/out_img1/0000095.png with resolution [1008, 744]\nSaved image to /kaggle/working/out_img1/0000049.png with resolution [1320, 912]\nSaved image to /kaggle/working/out_img1/0000018.png with resolution [1416, 944]\nSaved image to /kaggle/working/out_img1/0000055.png with resolution [1496, 992]\nSaved image to /kaggle/working/out_img1/0000028.png with resolution [1496, 992]\nSaved image to /kaggle/working/out_img1/0000061.png with resolution [744, 992]\nSaved image to /kaggle/working/out_img1/0000050.png with resolution [1368, 912]\nSaved image to /kaggle/working/out_img1/0000072.png with resolution [936, 1416]\nSaved image to /kaggle/working/out_img1/0000048.png with resolution [656, 840]\nSaved image to /kaggle/working/out_img1/0000071.png with resolution [1496, 992]\nSaved image to /kaggle/working/out_img1/0912.png with resolution [2040, 1080]\nSaved image to /kaggle/working/out_img1/0000078.png with resolution [1152, 864]\nSaved image to /kaggle/working/out_img1/0993.png with resolution [2040, 992]\nSaved image to /kaggle/working/out_img1/0937.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0000011.png with resolution [1320, 872]\nSaved image to /kaggle/working/out_img1/0000044.png with resolution [888, 584]\nSaved image to /kaggle/working/out_img1/0000002.png with resolution [632, 560]\nSaved image to /kaggle/working/out_img1/0000036.png with resolution [1320, 744]\nSaved image to /kaggle/working/out_img1/0000051.png with resolution [680, 680]\nSaved image to /kaggle/working/out_img1/0927.png with resolution [2040, 1016]\nSaved image to /kaggle/working/out_img1/0000074.png with resolution [1152, 768]\nSaved image to /kaggle/working/out_img1/0000064.png with resolution [1152, 864]\nSaved image to /kaggle/working/out_img1/0909.png with resolution [2040, 1368]\nSaved image to /kaggle/working/out_img1/0000045.png with resolution [864, 936]\nSaved image to /kaggle/working/out_img1/0000006.png with resolution [1440, 960]\nSaved image to /kaggle/working/out_img1/0949.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0991.png with resolution [2040, 1616]\nSaved image to /kaggle/working/out_img1/0970.png with resolution [2040, 1496]\nSaved image to /kaggle/working/out_img1/0943.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0957.png with resolution [2040, 1416]\nSaved image to /kaggle/working/out_img1/0000052.png with resolution [1616, 984]\nSaved image to /kaggle/working/out_img1/0922.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0955.png with resolution [1352, 2040]\nSaved image to /kaggle/working/out_img1/0953.png with resolution [2040, 1368]\nSaved image to /kaggle/working/out_img1/0000094.png with resolution [1280, 848]\nSaved image to /kaggle/working/out_img1/0942.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0935.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0000032.png with resolution [1008, 752]\nSaved image to /kaggle/working/out_img1/0000066.png with resolution [1016, 816]\nSaved image to /kaggle/working/out_img1/0930.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0918.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0000090.png with resolution [1320, 1976]\nSaved image to /kaggle/working/out_img1/0000092.png with resolution [696, 992]\nSaved image to /kaggle/working/out_img1/0952.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0932.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0977.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0000013.png with resolution [1280, 944]\nSaved image to /kaggle/working/out_img1/0906.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0000085.png with resolution [1496, 992]\nSaved image to /kaggle/working/out_img1/0976.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0916.png with resolution [2040, 1368]\nSaved image to /kaggle/working/out_img1/0962.png with resolution [2040, 1320]\nSaved image to /kaggle/working/out_img1/0946.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0000024.png with resolution [1248, 624]\nSaved image to /kaggle/working/out_img1/0000022.png with resolution [1224, 816]\nSaved image to /kaggle/working/out_img1/0924.png with resolution [1352, 2040]\nSaved image to /kaggle/working/out_img1/0000075.png with resolution [576, 1032]\nSaved image to /kaggle/working/out_img1/0963.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0000077.png with resolution [1296, 864]\nSaved image to /kaggle/working/out_img1/0934.png with resolution [1352, 2040]\nSaved image to /kaggle/working/out_img1/0000080.png with resolution [1392, 920]\nSaved image to /kaggle/working/out_img1/0938.png with resolution [2040, 1352]\nSaved image to /kaggle/working/out_img1/0000073.png with resolution [1344, 752]\nSaved image to /kaggle/working/out_img1/0931.png with resolution [2040, 1352]\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"import torch\nfrom torchvision.transforms import ToPILImage\n\nimport torch\nfrom torchvision.transforms import ToPILImage\nfrom PIL import Image\n\ndef save_tensor_as_png(tensor, file_path=\"output.png\", resolution=(256, 256)):\n    \"\"\"\n    Save a tensor as a PNG image with a specified resolution.\n\n    Args:\n        tensor (torch.Tensor): Tensor of shape (3, H, W).\n        file_path (str): File path to save the image.\n        resolution (tuple): Target resolution (width, height) in pixels.\n    \"\"\"\n    # Validate tensor shape\n    if tensor.ndim != 3 or tensor.shape[0] != 3:\n        raise ValueError(\"Tensor must have shape (3, H, W)\")\n\n    # Denormalize if necessary (from [-1, 1] to [0, 1])\n    if tensor.min() < 0 or tensor.max() <= 1:\n        tensor = (tensor * 0.5) + 0.5\n\n    # Clamp values to [0, 1] range\n    tensor = torch.clamp(tensor, 0, 1)\n\n    # Convert to PIL image\n    image = ToPILImage()(tensor)\n\n    # Resize to specified resolution\n    image = image.resize(resolution, Image.LANCZOS)\n\n    # Save the image\n    image.save(file_path)\n    print(f\"Saved image to {file_path} with resolution {resolution}\")\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T16:02:41.927745Z","iopub.execute_input":"2025-03-22T16:02:41.928103Z","iopub.status.idle":"2025-03-22T16:02:41.933901Z","shell.execute_reply.started":"2025-03-22T16:02:41.928075Z","shell.execute_reply":"2025-03-22T16:02:41.933062Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"mkdir out_img1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T16:02:50.463521Z","iopub.execute_input":"2025-03-22T16:02:50.463830Z","iopub.status.idle":"2025-03-22T16:02:50.623122Z","shell.execute_reply.started":"2025-03-22T16:02:50.463805Z","shell.execute_reply":"2025-03-22T16:02:50.622016Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"#import shutil\n\n#shutil.make_archive(\"/kaggle/working/team14\", 'zip', \"/kaggle/working/out_img1\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T16:07:18.552080Z","iopub.execute_input":"2025-03-22T16:07:18.552392Z","iopub.status.idle":"2025-03-22T16:07:30.448786Z","shell.execute_reply.started":"2025-03-22T16:07:18.552369Z","shell.execute_reply":"2025-03-22T16:07:30.447981Z"}},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/team14.zip'"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}